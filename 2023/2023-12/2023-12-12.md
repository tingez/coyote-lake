
# 2023-12-12 MoE: Joint modeling with divide-and-conquer strategy

Recently, more and more people are talking about MoE technique. In my point of view, MoE is a joint modeling technique with divide-and-conquer strategy
First, MoE intends to introduce different exports to resolve different tasks, just like divide-and-conquer, according to some kind of policy, divide the task into different small subtasks, and dispatch them to different dedicated exports, assemble the result of subtasks to resolve the original task. This strategy is widely adopted by industry. It is easy to isolate the issues and evolve the system performance engineeringly.
Second, MoE is joint modeling technique. In general, MoE replaces the dense FFN Layer with gate network and sparse MoE layers, train the whole network at the same time.
There's no such thing as a free lunch. It is a trade-off between bias and variance. The cost of more flexibility in model is training overfitting, training instabilities and uncertain quality caused by high variance during fine-tuning.
Several penalty items introduced in academic circle to mitigate this trade-off, such like router Z-loss etc. In my opinion, we still need more evidence about it. Of course, Mixtral-8x7b is a very good one.
