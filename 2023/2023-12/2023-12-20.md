
# 2023-12-20  An In-depth Look at Gemini's Language Abilities(paper reading)
  paper: https://arxiv.org/abs/2312.11444
![llm radar plot](/2023/2023-12/attachments/2023_12_20_llm_gemini.png)
 1. Some Takeways:
    1. The Gemini Pro model, generally achieves accuracy that is comparable but somewhat inferior to GPT 3.5 Turbo, and much worse than GPT 4. It outperforms Mixtral on every task that we examined.
    1. During the evaluation in some cases Gemini Pro blocks some questions, particularly in the case of potentially illegal or sensitive material.
    1. The result of MMLU indicate that Gemini has not been heavily instruction-tuned towards solving multiple-choice questions, which can cause models to be biased with respect to answer ordering.
    1. Based on result of BIG-Bench Hard, examined accuracy by the length of the question, Gemini Pro underperformed on longer, more complex questions while the GPT models were more robust to this.
    1. Web Agents task based on WebArena seems a little hard for LLM
