
# 2023-12-09 Why compared to ridge regression, lasso results in coefficient estimation exactly equal to zero?
aquantance with L1 and L2 nomalization to imporve linear model and least square performance, but not very familiar with the reason why lasso(L1 normalizaiotn) can exactly reduce the some of parameters to zero while ridge regression only can reduce to a small value.

According to An Introduction to Statistical Learning with Applications in Python, in 6.2.2
For a dataset with n observations and p predicators. 
ridge regression cost function is defined as the following:
```math
\begin{equation}\tag{1}
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_jx_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\beta_j^2}
\end{equation}
```
lasso cost function as following:

```math
\begin{equation}\tag{2}
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_jx_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\left|\beta_j\right|}
\end{equation}
```

One can show that the ridge regression coefficient estimates solve the problems:
```math
\begin{equation}\tag{3}
\min_{\beta}{\left\{\sum_{i=1}^n {\left( y_i - \beta_0 - \sum_{j=1}^p {\beta_jx_{ij}} \right)^2 } \right\}} \text{subject to}\sum_{j=1}^p{\beta_j^2} \leq s
\end{equation}
```
and  the corresponding lasso is
```math
\begin{equation}\tag{4}
\min_{\beta}{\left\{\sum_{i=1}^n {\left( y_i - \beta_0 - \sum_{j=1}^p {\beta_jx_{ij}} \right)^2 } \right\}} \text{subject to}\sum_{j=1}^p{\left|\beta_j\right|} \leq s
\end{equation}
```
When p = 2, then (4) indicates that the lasso coefcient estimates have the smallest RSS out of all points that lie within the diamond defned by |β,,1,,| + |β,,2,,| ≤ s.
Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defned by β,,1,,^2^ + β,,2,,^2^ ≤ s

 A geometric interpretation is given:
Figure 6.7 illustrates the situation. 
The least squares solution is marked as $\hat{β}, while the blue diamond and circle represent the lasso and ride regression constraints respectively.


