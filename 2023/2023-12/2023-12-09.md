
# 2023-12-09 Why compared to ridge regression, lasso results in coefficient estimation exactly equal to zero?
aquantance with L1 and L2 nomalization to imporve linear model and least square performance, but not very familiar with the reason why lasso(L1 normalizaiotn) can exactly reduce the some of parameters to zero while ridge regression only can reduce to a small value.

According to An Introduction to Statistical Learning with Applications in Python, in 6.2.2, A geometric interpretation is given:

$\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_jx_{ij}} \right)} + \lambda \sum_{j=1}^p{\left|\beta_j\right|}$
Figure 6.7 illustrates the situation. 
The least squares solution is marked as $\hat{Î²}, while the blue diamond and circle represent the lasso and ride regression constraints respectively.


