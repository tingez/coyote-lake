
# 2023-12-09 Why compared to ridge regression, lasso results in coefficient estimation exactly equal to zero?
aquantance with L1 and L2 nomalization to imporve linear model and least square performance, but not very familiar with the reason why lasso(L1 normalizaiotn) can exactly reduce the some of parameters to zero while ridge regression only can reduce to a small value.

According to An Introduction to Statistical Learning with Applications in Python, in 6.2.2
For a dataset with n observations and p predicators. 
ridge regression cost function is defined as the following:
```math
\begin{equation}\tag{1}
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_jx_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\beta_j^2}
\end{equation}
```
lasso cost function as following:

```math
\begin{equation}\tag{2}
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_jx_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\left|\beta_j\right|}
\end{equation}
```

One can show that the ridge regression coefficient estimates solve the problems:
```math
\begin{equation}\tag{3}
\min_{\beta}{\left\{\sum_{i=1}^n {\left( y_i - \beta_0 - \sum_{j=1}^p {\beta_jx_{ij}} \right)^2 } \right\}} \text{subject to}\sum_{j=1}^p{\beta_j^2} \leq s
\end{equation}
```
and  the corresponding lasso is
```math
\begin{equation}\tag{4}
\min_{\beta}{\left\{\sum_{i=1}^n {\left( y_i - \beta_0 - \sum_{j=1}^p {\beta_jx_{ij}} \right)^2 } \right\}} \text{subject to}\sum_{j=1}^p{\left|\beta_j\right|} \leq s
\end{equation}
```
When p = 2, then (4) indicates that the lasso coefcient estimates have the smallest RSS out of all points that lie within the diamond defned by $\left|\beta_1\right| + \left|\beta_2\right| \leq s$.  Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defned by $\beta_1^2 + \beta_2^2 \leq s$.

Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? The formulations (3) and (4) can be used to shed light on the issue
The following figure illustrates the situation. The least squares solution is marked as $\hat{\beta}$, while the blue diamond and circle represent the lasso and ridge regression constraints in (3) and (4), respectively. If s is sufciently large, then the constraint regions will contain $\hat{\beta}$, and so the ridge regression and lasso estimates will be the same as the least squares estimates
![Contours of the error and constraint functions for the lasso (left) and ridge regression (right)](/2023/2023-12/attachments/2023_12_09_figure_6_7.png)

Each of the ellipses centered around $\hat{\beta}$ represents a contour: this means that all of the points on a particular ellipse have the same RSS value. As the ellipses expand away from the least squares coefficient estimates, the RSS increases. Equations(3) and (4) indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.
 
However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefcients will equal zero. In higher dimensions, many of the coefcient estimates may equal zero simultaneously. When p is equal to or larger than 3,  the key ideas depicted in this figure still hold.
